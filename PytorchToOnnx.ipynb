{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f2526574-e74c-421d-a5ed-36b713a8e36e",
      "metadata": {
        "id": "f2526574-e74c-421d-a5ed-36b713a8e36e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3f7e878c-dc81-4da7-ae18-6c8bcf65ff0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3f7e878c-dc81-4da7-ae18-6c8bcf65ff0c",
        "outputId": "d5ad6212-05ba-4882-a134-f05afdf479a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be44e87c-fe88-4f41-8ec4-4b4c75453d25",
      "metadata": {
        "id": "be44e87c-fe88-4f41-8ec4-4b4c75453d25"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2554a063-2e47-4949-896c-610203652a17",
      "metadata": {
        "id": "2554a063-2e47-4949-896c-610203652a17"
      },
      "outputs": [],
      "source": [
        "def train( model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 12000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "77cdba70-4657-4d7b-9ffd-47d7ae2bda68",
      "metadata": {
        "id": "77cdba70-4657-4d7b-9ffd-47d7ae2bda68"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                  transform=transforms.Compose([\n",
        "                  transforms.RandomAffine(\n",
        "                  degrees=30, translate=(0.5, 0.5), scale=(0.25, 1),\n",
        "                  shear=(-30, 30, -30, 30)),\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.1307,), (0.3081,))\n",
        "            ])), batch_size=32, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])),  batch_size=32, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3bb96082-981d-42e7-b7c9-ea6ced2908b0",
      "metadata": {
        "id": "3bb96082-981d-42e7-b7c9-ea6ced2908b0"
      },
      "outputs": [],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx4nVZBe9M6S",
        "outputId": "19b23144-0486-4a86-a1e6-35d023a7c03b"
      },
      "id": "Nx4nVZBe9M6S",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "dc1582b3-0335-4ea9-9701-0dc1f81f8468",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1582b3-0335-4ea9-9701-0dc1f81f8468",
        "outputId": "84b43345-012e-48e1-d8e9-204f26423323"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299779\n",
            "\n",
            "Test set: Average loss: 1.6225, Accuracy: 4510/10000 (45%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.031099\n",
            "\n",
            "Test set: Average loss: 1.0097, Accuracy: 7143/10000 (71%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.984479\n",
            "\n",
            "Test set: Average loss: 0.8686, Accuracy: 7108/10000 (71%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.745221\n",
            "\n",
            "Test set: Average loss: 0.6131, Accuracy: 8409/10000 (84%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.719652\n",
            "\n",
            "Test set: Average loss: 0.5608, Accuracy: 8632/10000 (86%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.730396\n",
            "\n",
            "Test set: Average loss: 0.4884, Accuracy: 8770/10000 (88%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.972951\n",
            "\n",
            "Test set: Average loss: 0.4832, Accuracy: 8751/10000 (88%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.494474\n",
            "\n",
            "Test set: Average loss: 0.4967, Accuracy: 8683/10000 (87%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.475579\n",
            "\n",
            "Test set: Average loss: 0.4715, Accuracy: 8713/10000 (87%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.679146\n",
            "\n",
            "Test set: Average loss: 0.4471, Accuracy: 8824/10000 (88%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.290392\n",
            "\n",
            "Test set: Average loss: 0.4258, Accuracy: 8895/10000 (89%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.476294\n",
            "\n",
            "Test set: Average loss: 0.4351, Accuracy: 8863/10000 (89%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.257190\n",
            "\n",
            "Test set: Average loss: 0.4152, Accuracy: 8920/10000 (89%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.612766\n",
            "\n",
            "Test set: Average loss: 0.4182, Accuracy: 8902/10000 (89%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.726386\n",
            "\n",
            "Test set: Average loss: 0.4170, Accuracy: 8893/10000 (89%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.128511\n",
            "\n",
            "Test set: Average loss: 0.4247, Accuracy: 8871/10000 (89%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.442281\n",
            "\n",
            "Test set: Average loss: 0.4189, Accuracy: 8902/10000 (89%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.459029\n",
            "\n",
            "Test set: Average loss: 0.4157, Accuracy: 8910/10000 (89%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.235932\n",
            "\n",
            "Test set: Average loss: 0.4178, Accuracy: 8915/10000 (89%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.337774\n",
            "\n",
            "Test set: Average loss: 0.4182, Accuracy: 8904/10000 (89%)\n",
            "\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 1.558158\n",
            "\n",
            "Test set: Average loss: 0.4171, Accuracy: 8911/10000 (89%)\n",
            "\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 1.673155\n",
            "\n",
            "Test set: Average loss: 0.4170, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.448277\n",
            "\n",
            "Test set: Average loss: 0.4178, Accuracy: 8912/10000 (89%)\n",
            "\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 1.684077\n",
            "\n",
            "Test set: Average loss: 0.4162, Accuracy: 8910/10000 (89%)\n",
            "\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.425159\n",
            "\n",
            "Test set: Average loss: 0.4170, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.315307\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8912/10000 (89%)\n",
            "\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.384476\n",
            "\n",
            "Test set: Average loss: 0.4168, Accuracy: 8911/10000 (89%)\n",
            "\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 1.168207\n",
            "\n",
            "Test set: Average loss: 0.4170, Accuracy: 8912/10000 (89%)\n",
            "\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.429592\n",
            "\n",
            "Test set: Average loss: 0.4171, Accuracy: 8911/10000 (89%)\n",
            "\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 1.550889\n",
            "\n",
            "Test set: Average loss: 0.4170, Accuracy: 8911/10000 (89%)\n",
            "\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.553242\n",
            "\n",
            "Test set: Average loss: 0.4172, Accuracy: 8911/10000 (89%)\n",
            "\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 2.155080\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8910/10000 (89%)\n",
            "\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 1.711032\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8910/10000 (89%)\n",
            "\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 1.438911\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8910/10000 (89%)\n",
            "\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 1.653462\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 1.265778\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 1.411673\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 1.930895\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 1.358656\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 1.172479\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 1.299055\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 1.315421\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 1.292096\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 1.348704\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 1.519244\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 1.305448\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 1.699464\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 1.490541\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 1.497559\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 1.569184\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 51 [0/60000 (0%)]\tLoss: 1.085359\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 52 [0/60000 (0%)]\tLoss: 1.487833\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 53 [0/60000 (0%)]\tLoss: 1.446829\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 54 [0/60000 (0%)]\tLoss: 1.301343\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 55 [0/60000 (0%)]\tLoss: 1.267959\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 56 [0/60000 (0%)]\tLoss: 1.033627\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 57 [0/60000 (0%)]\tLoss: 1.354952\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 58 [0/60000 (0%)]\tLoss: 1.216811\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 59 [0/60000 (0%)]\tLoss: 1.453286\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 60 [0/60000 (0%)]\tLoss: 1.243765\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 61 [0/60000 (0%)]\tLoss: 1.324454\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 62 [0/60000 (0%)]\tLoss: 1.200236\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 63 [0/60000 (0%)]\tLoss: 1.718098\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 64 [0/60000 (0%)]\tLoss: 1.491348\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 65 [0/60000 (0%)]\tLoss: 1.713438\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 66 [0/60000 (0%)]\tLoss: 1.163799\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 67 [0/60000 (0%)]\tLoss: 1.837108\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 68 [0/60000 (0%)]\tLoss: 1.030397\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 69 [0/60000 (0%)]\tLoss: 1.632519\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n",
            "Train Epoch: 70 [0/60000 (0%)]\tLoss: 1.725852\n",
            "\n",
            "Test set: Average loss: 0.4173, Accuracy: 8909/10000 (89%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 71):\n",
        "    train( model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "53f1c091-3065-4b2a-956d-75c0e5532189",
      "metadata": {
        "id": "53f1c091-3065-4b2a-956d-75c0e5532189"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"pytorch_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3d4b12d1-116d-4feb-a62a-436886e29a8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d4b12d1-116d-4feb-a62a-436886e29a8c",
        "outputId": "5b2046ed-c32c-465e-fc7e-9d530664a17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ],
      "source": [
        "dummy_input = torch.randn(1,1,28,28).to(device)\n",
        "torch.onnx.export(model, dummy_input, 'onnx_model.onnx', verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session = onnxruntime.InferenceSession('./onnx_model.onnx')"
      ],
      "metadata": {
        "id": "4K1DMAqJUXoH"
      },
      "id": "4K1DMAqJUXoH",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = session.get_inputs()[0].name\n",
        "print(input_name)\n",
        "output_name = session.get_outputs()[0].name\n",
        "print(output_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXc5noVcUtcV",
        "outputId": "76db04e5-bd69-4b0d-f7da-300b193fccd0"
      },
      "id": "AXc5noVcUtcV",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.1\n",
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.get_inputs()[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl2bEj4XU5ZV",
        "outputId": "6a17bcc4-26f4-424d-e41e-cedfe8929b8c"
      },
      "id": "Fl2bEj4XU5ZV",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 28, 28]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test_loader:\n",
        "    sample = i\n",
        "    break"
      ],
      "metadata": {
        "id": "YlXdrCkdVAuz"
      },
      "id": "YlXdrCkdVAuz",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sample[0][0].numpy()\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxO4pEo9Vqvb",
        "outputId": "4ee0b99d-09aa-470a-a1d9-00b3a4642a57"
      },
      "id": "WxO4pEo9Vqvb",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.33511534,\n",
              "         -0.04236595,  2.286901  ,  2.7960303 ,  1.154088  ,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296,  0.69587165,  2.1978033 ,\n",
              "          2.783302  ,  2.783302  ,  2.783302  ,  0.98862094,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.2205612 ,  1.4977505 ,  2.7960303 ,  2.783302  ,\n",
              "          2.783302  ,  2.783302  ,  2.783302  ,  0.33948106,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296,  0.11037287,\n",
              "          1.306827  ,  2.783302  ,  2.7960303 ,  2.783302  ,\n",
              "          2.783302  ,  2.783302  ,  2.3378139 ,  0.18674228,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.15692005,  2.4396398 ,\n",
              "          2.783302  ,  2.783302  ,  2.7960303 ,  2.783302  ,\n",
              "          2.783302  ,  1.968695  , -0.15692005, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296,  0.73405635,  2.286901  ,  2.783302  ,\n",
              "          1.7268586 ,  0.33948106,  2.7960303 ,  2.783302  ,\n",
              "          2.6560197 ,  0.33948106, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          0.7213281 ,  2.6432915 ,  2.783302  ,  1.854141  ,\n",
              "          0.05945994, -0.42421296,  2.7960303 ,  2.783302  ,\n",
              "          2.3250856 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          1.8795974 ,  2.783302  ,  1.2686423 , -0.3605718 ,\n",
              "         -0.42421296, -0.42421296,  2.7960303 ,  2.783302  ,\n",
              "          1.0140774 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          1.2177293 ,  2.783302  ,  2.0323362 , -0.2205612 ,\n",
              "         -0.42421296,  0.9758927 ,  2.7960303 ,  2.6560197 ,\n",
              "          0.2758399 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.01690946,  2.4269114 ,  2.783302  ,  2.0705209 ,\n",
              "          0.45403522,  2.5796502 ,  2.7960303 ,  1.7395868 ,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296,  0.02127524,  1.6250328 ,  2.783302  ,\n",
              "          2.7960303 ,  2.7960303 ,  2.8214867 ,  0.9631645 ,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296,  2.0705209 ,\n",
              "          2.7578456 ,  2.783302  ,  2.7960303 ,  0.95043623,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          2.4905527 ,  2.783302  ,  2.7960303 ,  0.95043623,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          2.4905527 ,  2.783302  ,  2.7960303 ,  1.968695  ,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.11873534,\n",
              "          2.566922  ,  2.783302  ,  2.7960303 ,  2.5541937 ,\n",
              "         -0.13146357, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296,  0.45403522,\n",
              "          2.6942043 ,  2.783302  ,  1.1922727 ,  2.783302  ,\n",
              "          1.2304575 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          2.4905527 ,  2.783302  , -0.25874594,  2.783302  ,\n",
              "          2.3250856 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          2.4905527 ,  2.783302  ,  2.3759985 ,  2.783302  ,\n",
              "          2.3250856 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "          1.103175  ,  2.783302  ,  2.7960303 ,  2.783302  ,\n",
              "          1.6632175 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.3605718 ,  2.6560197 ,  2.7960303 ,  1.7523152 ,\n",
              "         -0.3605718 , -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296],\n",
              "        [-0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
              "         -0.42421296, -0.42421296, -0.42421296, -0.42421296]]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = sample[1][0]\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5khRcws-V074",
        "outputId": "33d68b06-a7bd-4d0e-b5df-cd30d9c7abcc"
      },
      "id": "5khRcws-V074",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2V7qLJeW1qF",
        "outputId": "24e3b794-1240-42f1-9e21-805203b42ba3"
      },
      "id": "I2V7qLJeW1qF",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.expand_dims(x, axis=0)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy1vp70XW6nS",
        "outputId": "9429ed2a-32d3-4c11-bf8d-efea53b8133e"
      },
      "id": "wy1vp70XW6nS",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = session.run([output_name], {input_name: x})"
      ],
      "metadata": {
        "id": "_Eo6NOrTV4_C"
      },
      "id": "_Eo6NOrTV4_C",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-xgUBkTV9Lg",
        "outputId": "085db252-159c-4f75-b9e1-d0a55b37f5c9"
      },
      "id": "c-xgUBkTV9Lg",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "torchenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}